# Explainable Image Captioning with SHAP and PyTorch

This project demonstrates how to explain the outputs of an image captioning model using the SHAP framework. The main objective is to understand how different parts of an input image contribute to the generated captions by using SHAP's partition explainer.

## Overview

Helping SHAP documentation, this notebook utilizes a pre-trained open-source model from [ImageCaptioning.pytorch](https://github.com/ruotianluo/ImageCaptioning.pytorch). The SHAP framework is used to generate explanations for the captions by masking image segments and analyzing the corresponding changes in the caption predictions.

The project uses:
- **PyTorch** for loading the pre-trained image captioning model.
- **SHAP** to explain the predictions of the image captioning model by masking various parts of the image.

## Features

- **Explainability**: The project leverages SHAP's partitioning technique to explain how different image regions influence the caption generated by the model.
- **Pre-trained model**: The project uses a pre-trained model based on ResNet101 features for captioning.
- **Custom Masking Techniques**: Multiple masking techniques such as inpainting and blurring are used to assess how each affects the image-caption relationship.
- **Multi-class similarity scoring**: Using the DistilBART transformer language model for alignment scoring between masked and original image captions.

## Requirements

- Python 3.7+
- **PyTorch**
- **SHAP**
- **NumPy**
- **Matplotlib**
- Pre-trained captioning model weights (`model-best.pth`, `infos_fc_nsc-best.pkl`, `resnet101` weights)

## Setup

1. Clone the repository and navigate to the directory:
    ```bash
    git clone https://github.com/your-username/explainable-image-captioning.git
    cd explainable-image-captioning
    ```

2. Install the required dependencies:
    ```bash
    pip install -r requirements.txt
    ```

3. Download the pre-trained models:
    - Download the model weights and place them in the appropriate directories:
      - [Pre-trained model](https://drive.google.com/drive/folders/1OsB_jLDorJnzKz6xsOfk1n493P3hwOP0) (`model-best.pth`, `infos_fc_nsc-best.pkl`)
      - [ResNet101 weights](https://drive.google.com/drive/folders/0B7fNdx_jAqhtbVYzOURMdDNHSGM) (`resnet101`)
      
4. Set up the project:
    ```bash
    python -m pip install -e .
    ```

5. (Optional) Run the evaluation script:
    ```bash
    python tools/eval.py --model model-best.pth --infos_path infos_fc_nsc-best.pkl --image_folder test_images --num_images 10
    ```

## Usage

1. Load the notebook in Jupyter:
    ```bash
    jupyter notebook
    ```

2. Run the cells to load the image captioning model, SHAP explainer, and evaluate image captions.

3. For SHAP explanations:
    - Run the SHAP explainer over test images.
    - Visualize the image segments (super pixels) and their contribution to the caption.

## Example Results

- SHAP will generate a visualization showing the most important regions in the image that contributed to the caption generation.

## Future Improvements

- **Semantic segmentation**: Improve explanations by using semantic segmentation instead of axis-aligned partitioning for image masking.
- **Custom scoring**: Integrate the captioning modelâ€™s own language head for scoring instead of using external models like DistilBART.

## Acknowledgments

- The open-source image captioning model is from [Ruotian Luo's ImageCaptioning.pytorch](https://github.com/ruotianluo/ImageCaptioning.pytorch).
- SHAP explainability framework by [SHAP](https://github.com/slundberg/shap).

---

This README provides an overview of the project, setup instructions, usage, and potential future improvements. You can modify or expand it based on the details of your project.
